{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036b89f0-3858-48e0-bb7f-3b13889c588e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import av\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import gc\n",
    "\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModel, VideoLlavaForConditionalGeneration, VideoLlavaProcessor\n",
    "from PIL import Image\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Current device:\", torch.cuda.current_device() if torch.cuda.is_available() else \"CPU only\")\n",
    "print(\"Device name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11f7efda-09bd-44d9-8b03-14d9e5eb2a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/common/home/projectgrps/CS707/CS707G3/.cache/huggingface/hub\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "import huggingface_hub\n",
    "print(huggingface_hub.constants.HF_HUB_CACHE)\n",
    "\n",
    "CACHE_DIR = Path(huggingface_hub.constants.HF_HUB_CACHE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad49ce4-6fe4-4c20-88f5-80b93b013e95",
   "metadata": {},
   "source": [
    "##### Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f7de25-e36b-4d98-858e-b59b9bab96c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = CACHE_DIR / \"models--LanguageBind--Video-LLaVA-7B-hf\" / \"snapshots\"\n",
    "# MODEL_PATH = \"/common/public/InternVL2-8B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e33c4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install ipykernel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5d100c4-142d-4e23-86b7-80447c5cbc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_video_pyav(container, indices):\n",
    "    '''\n",
    "    Decode the video with PyAV decoder.\n",
    "    Args:\n",
    "        container (`av.container.input.InputContainer`): PyAV container.\n",
    "        indices (`list[int]`): List of frame indices to decode.\n",
    "    Returns:\n",
    "        result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n",
    "    '''\n",
    "    frames = []\n",
    "    container.seek(0)\n",
    "    start_index = indices[0]\n",
    "    end_index = indices[-1]\n",
    "    for i, frame in enumerate(container.decode(video=0)):\n",
    "        if i > end_index:\n",
    "            break\n",
    "        if i >= start_index and i in indices:\n",
    "            frames.append(frame)\n",
    "    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc2ed9c1-f698-4b3e-9391-0338462571e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4330317b6dc45dd97a2b8a55a29bbf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = VideoLlavaForConditionalGeneration.from_pretrained(\n",
    "    \"LanguageBind/Video-LLaVA-7B-hf\", \n",
    "    dtype=torch.float16, \n",
    "    # device_map=\"auto\",\n",
    "    # attn_implementation=\"flash_attention_2\"\n",
    "    # attn_implementation=\"sdpa\"\n",
    ").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "processor = VideoLlavaProcessor.from_pretrained(\"LanguageBind/Video-LLaVA-7B-hf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8905154b-b528-462c-8875-46c6891950c9",
   "metadata": {},
   "source": [
    "##### Create a Batch of questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6df6c9-e9a8-4d96-9f1d-6effbc5e77d4",
   "metadata": {},
   "source": [
    "https://huggingface.co/docs/transformers/model_doc/video_llava#mixed-media-mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0526cbad-415c-44a5-afc1-c59927a4b92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"What is happening in this video?\",\n",
    "    \"How many people are in the scene?\",\n",
    "    \"What is the person doing at the beginning?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0a4f184-de3e-47b3-a10f-c6b3c9b56549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER: \n",
      "What is happening in this video? ASSISTANT: In the video, a man is sitting on a couch with his arms crossed, while a woman is sitting on a chair next to him. They are both watching TV, and the man is talking to the woman.\n",
      "Script execution time: 1.3337 seconds\n"
     ]
    }
   ],
   "source": [
    "prompt = \"USER: <video>\\nWhat is happening in this video? ASSISTANT:\"\n",
    "video_path = r\"./src/data/video_clip/0102_scene_000_central_perk.mp4\"\n",
    "container = av.open(video_path)\n",
    "\n",
    "# sample uniformly 8 frames from the video\n",
    "total_frames = container.streams.video[0].frames\n",
    "indices = np.arange(0, total_frames, total_frames / 8).astype(int)\n",
    "clip = read_video_pyav(container, indices)\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "inputs = processor(text=prompt, videos=clip, return_tensors=\"pt\")\n",
    "inputs = inputs.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Generate\n",
    "generate_ids = model.generate(**inputs, max_new_tokens=256)\n",
    "output_text = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "end_time = time.perf_counter()\n",
    "elapsed_time = end_time - start_time\n",
    "print(output_text)\n",
    "print(f\"Script execution time: {elapsed_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3c564d-e30d-4fba-8938-f86a49125303",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
